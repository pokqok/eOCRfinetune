# run_all_preprocessing.py (ë³€ìˆ˜ ë²”ìœ„ ì˜¤ë¥˜ ìˆ˜ì • ìµœì¢…ë³¸)

import os
import glob
import json
import shutil
import pandas as pd
from PIL import Image
from tqdm import tqdm
import sys
import random

# --- [ì‚¬ìš©ì ì„¤ì •] ìµœì¢… ë°ì´í„° ë ˆì‹œí”¼ ---
DATA_RECIPE = {
    'ê¸€ì': 80000,
    'ë‹¨ì–´': 120000,
    'qmnist': 120000
}

def verify_step(step_name, image_dir, label_file, image_prefix):
    """ì¤‘ê°„ ê²€ì¦ í•¨ìˆ˜"""
    print("\n" + "-"*20 + f" [{step_name}] ì¤‘ê°„ ê²€ì¦ " + "-"*20)
    try:
        image_count = 0
        if os.path.exists(image_dir):
            image_count = len([f for f in os.listdir(image_dir) if f.startswith(image_prefix)])
        print(f"  - ìƒì„±ëœ '{image_prefix}...' ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜: {image_count:,} ê°œ")

        label_count = 0
        if os.path.exists(label_file):
            with open(label_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if os.path.basename(line.strip().split('\t')[0]).startswith(image_prefix):
                        label_count += 1
        print(f"  - ì¶”ê°€ëœ '{image_prefix}...' ë¼ë²¨ ìˆ˜: {label_count:,} ê°œ")
        
        if image_count > 0 and label_count > 0:
            print("  - âœ… ê²€ì¦ ê²°ê³¼: ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.")
        else:
            print("  - âš ï¸  ê²½ê³ : í•´ë‹¹ ìœ í˜•ì˜ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¼ë²¨ì´ ê¸°ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"  - âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("-"*(42 + len(step_name)))


def process_data(data_type, target_count, paths):
    """ì§€ì •ëœ ìœ í˜•ì˜ ë°ì´í„°ë§Œ ì „ì²˜ë¦¬í•˜ëŠ” ë²”ìš© í•¨ìˆ˜"""
    
    print(f"\n{'='*15} {data_type} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ (ëª©í‘œ: {target_count:,}ê°œ) {'='*15}")
    
    source_items = []
    # ì†ŒìŠ¤ ëª©ë¡ ìƒì„±
    if data_type == 'qmnist':
        mnist_label_file = os.path.join(paths['AIHUB_SOURCE_DIR'], "qmnist_labels.csv")
        if os.path.exists(mnist_label_file):
            source_items = pd.read_csv(mnist_label_file).to_dict('records')
    else: # AI Hub ë°ì´í„°
        source_items = glob.glob(os.path.join(paths['AIHUB_SOURCE_DIR'], '**', f'*{data_type}*/**/', '*.json'), recursive=True)
    
    if not source_items:
        print(f"'{data_type}' ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    # ì²˜ë¦¬ ì•ˆ ëœ í•­ëª© í•„í„°ë§ ë° ìƒ˜í”Œë§
    unprocessed_items = []
    for item in source_items:
        item_id = f"qmnist_{item['filename']}" if data_type == 'qmnist' else os.path.relpath(item, paths['AIHUB_SOURCE_DIR'])
        if item_id not in paths['completed_items']:
            unprocessed_items.append(item)
    
    items_to_process = random.sample(unprocessed_items, min(len(unprocessed_items), target_count))
    print(f"  - ì²˜ë¦¬ ëŒ€ìƒ {len(items_to_process):,}ê°œë¥¼ ì„ íƒí•˜ì—¬ ì§„í–‰í•©ë‹ˆë‹¤.")

    # ë©”ì¸ ë£¨í”„
    for item in tqdm(items_to_process, desc=f"{data_type} ì²˜ë¦¬ ì¤‘"):
        try:
            item_id = "" # item_id ì´ˆê¸°í™”
            if data_type == 'qmnist':
                filename, text = item['filename'], item['text']
                item_id = f"qmnist_{filename}"
                src_path = os.path.join(paths['AIHUB_SOURCE_DIR'], "qmnist_images", filename)
                new_name = f"qmnist_{filename}"
                dest_path = os.path.join(paths['CROP_IMAGE_DIR'], new_name)
                if not os.path.exists(src_path): continue
                
                shutil.move(src_path, dest_path)
                relative_path = os.path.join('images', os.path.basename(dest_path)).replace('\\', '/')
                with open(paths['LABEL_FILE_PATH'], 'a', encoding='utf-8') as f: f.write(f"{relative_path}\t{str(text)}\n")

            else: # AI Hub
                json_path = item
                item_id = os.path.relpath(json_path, paths['AIHUB_SOURCE_DIR'])
                with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)
                
                image_filename = data.get('image', {}).get('file_name')
                if not image_filename: continue
                image_folder_path = os.path.dirname(json_path).replace('[ë¼ë²¨]', '[ì›ì²œ]')
                src_path = os.path.join(image_folder_path, image_filename)
                if not os.path.exists(src_path): continue
                
                annotations = data.get('text', {}).get('word', [])
                if 'letter' in data['text']: annotations = [data['text']['letter']]
                if not annotations: continue
                
                label = "".join([c.get('value', '') for c in annotations]) if data_type == 'ë‹¨ì–´' else annotations[0].get('value', '')
                if not label.strip(): continue
                
                prefix = "hw_word_" if data_type == 'ë‹¨ì–´' else "hw_char_"
                new_name = f"{prefix}{os.path.splitext(os.path.basename(json_path))[0]}.png"
                dest_path = os.path.join(paths['CROP_IMAGE_DIR'], new_name)
                
                shutil.move(src_path, dest_path)
                
                relative_path = os.path.join('images', new_name).replace('\\', '/')
                with open(paths['LABEL_FILE_PATH'], 'a', encoding='utf-8') as f: f.write(f"{relative_path}\t{label.strip()}\n")
            
            with open(paths['LOG_FILE_PATH'], 'a', encoding='utf-8') as f: f.write(item_id + '\n')
            paths['completed_items'].add(item_id)
        except Exception as e:
            print(f"\nì˜¤ë¥˜: {item} ì²˜ë¦¬ ì¤‘ ë¬¸ì œ: {e}", file=sys.stderr)
            
    # ë‹¨ê³„ë³„ ê²€ì¦
    prefix_map = {'ê¸€ì': 'hw_char_', 'ë‹¨ì–´': 'hw_word_', 'qmnist': 'qmnist_'}
    if data_type in prefix_map:
        verify_step(data_type, paths['CROP_IMAGE_DIR'], paths['LABEL_FILE_PATH'], prefix_map[data_type])


def main():
    # --- ê²½ë¡œ ì„¤ì • ---
    print("[1/3] ê¸°ë³¸ ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤...")
    try:
        paths = {}
        paths['BASE_DIR'] = os.getcwd()
        paths['AIHUB_SOURCE_DIR'] = os.path.join(paths['BASE_DIR'], "ë‹¤ì–‘í•œ í˜•íƒœì˜ í•œê¸€ ë¬¸ì OCR")
        paths['ALL_DATA_DIR'] = os.path.join(paths['BASE_DIR'], "all_data")
        paths['CROP_IMAGE_DIR'] = os.path.join(paths['ALL_DATA_DIR'], "images")
        paths['LABEL_FILE_PATH'] = os.path.join(paths['ALL_DATA_DIR'], "labels.txt")
        paths['LOG_FILE_PATH'] = os.path.join(paths['BASE_DIR'], "final_preprocessing_log.txt")
        
        os.makedirs(paths['CROP_IMAGE_DIR'], exist_ok=True)
        if not os.path.exists(paths['AIHUB_SOURCE_DIR']):
            raise FileNotFoundError(f"ì†ŒìŠ¤ í´ë” ì—†ìŒ: {paths['AIHUB_SOURCE_DIR']}")
        print("âœ… ê²½ë¡œ ì„¤ì • ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ ê²½ë¡œ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}"); sys.exit()

    # --- ë¡œê·¸ ì¤€ë¹„ ---
    print("\n[2/3] ì‘ì—… ë¡œê·¸ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    try:
        with open(paths['LOG_FILE_PATH'], 'r', encoding='utf-8') as f:
            paths['completed_items'] = set(f.read().splitlines())
    except FileNotFoundError:
        paths['completed_items'] = set()
    print(f"âœ… ì´ {len(paths['completed_items'])}ê°œ í•­ëª©ì´ ì´ì „ì— ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- 3. ë°ì´í„° ìœ í˜•ë³„ë¡œ ì „ì²˜ë¦¬ ì‹¤í–‰ ---
    print("\n[3/3] ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    # 'form' ë°ì´í„°ëŠ” ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì œì™¸
    for data_type, target_count in DATA_RECIPE.items():
        process_data(data_type, target_count, paths)

    print("\n" + "="*50)
    print("ğŸ‰ ëª¨ë“  ë°ì´í„° ì¤€ë¹„ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰")
    print(f"ìµœì¢… ë°ì´í„° í´ë”: {paths['ALL_DATA_DIR']}")
    print("="*50)

if __name__ == '__main__':
    main()