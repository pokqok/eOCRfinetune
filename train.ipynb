{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ym6UK0O_4VQD",
        "outputId": "7c36d784-b173-4bd1-ec94-b6ccd405c67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (8.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading lmdb-1.6.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=9303d4b51410a788b60e2f13ee66578e57614af3942475f548787e3609c58053\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: lmdb, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed fire-0.7.0 lmdb-1.6.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install fire lmdb opencv-python natsort nltk torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ByOuKl8H5RjA",
        "outputId": "d4a199ea-50b7-4015-ba20-d849cd6e8814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.11/dist-packages (1.6.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (8.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install lmdb pillow torchvision nltk natsort fire\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NG5Iph0O-ALW",
        "outputId": "de06f6fa-9a12-4cae-977a-28c52aa4b9c1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with '.venv (Python 3.10.8)' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'e:/download/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/clovaai/deep-text-recognition-benchmark.git\n",
        "%cd deep-text-recognition-benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 구조 저장 완료: directory_structure.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def write_tree_structure(root_path, output_file):\n",
        "    ignored_dirs = {'.venv', '__pycache__', '.git', '.idea', '.vscode', 'env'}\n",
        "    ignored_exts = {'.pyc', '.pkl', '.pt', '.log', '.swp'}\n",
        "\n",
        "    def is_leaf_dir(path):\n",
        "        \"\"\"하위 디렉토리가 없는 경우 True\"\"\"\n",
        "        try:\n",
        "            for entry in os.listdir(path):\n",
        "                full_path = os.path.join(path, entry)\n",
        "                if os.path.isdir(full_path) and entry not in ignored_dirs:\n",
        "                    return False\n",
        "        except:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def walk_dir(current_path, prefix=''):\n",
        "        try:\n",
        "            entries = sorted(os.listdir(current_path))\n",
        "        except PermissionError:\n",
        "            return\n",
        "\n",
        "        entries = [e for e in entries if not (\n",
        "            e in ignored_dirs or\n",
        "            any(e.endswith(ext) for ext in ignored_exts)\n",
        "        )]\n",
        "\n",
        "        dirs = [e for e in entries if os.path.isdir(os.path.join(current_path, e))]\n",
        "        for i, name in enumerate(dirs):\n",
        "            path = os.path.join(current_path, name)\n",
        "            connector = '└── ' if i == len(dirs) - 1 else '├── '\n",
        "            lines.append(f\"{prefix}{connector}{name}\")\n",
        "            if not is_leaf_dir(path):\n",
        "                new_prefix = prefix + ('    ' if i == len(dirs) - 1 else '│   ')\n",
        "                walk_dir(path, new_prefix)\n",
        "\n",
        "    lines = [f\"📁 디렉토리 구조: {os.path.abspath(root_path)}\\n\"]\n",
        "    walk_dir(root_path)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "        print(f\"✅ 구조 저장 완료: {output_file}\")\n",
        "\n",
        "# 사용 예시\n",
        "if __name__ == \"__main__\":\n",
        "    project_root = os.path.abspath('.')  # 또는 예: 'E:/download/easyocr'\n",
        "    output_txt = \"directory_structure.txt\"\n",
        "    write_tree_structure(project_root, output_txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_PA64bN9hAy",
        "outputId": "ecca8a7f-2322-49de-9915-4fd250df883a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: e:\\download\n",
            "Written 1000 / 326670\n",
            "Written 2000 / 326670\n",
            "Written 3000 / 326670\n",
            "Written 4000 / 326670\n",
            "Written 5000 / 326670\n",
            "Written 6000 / 326670\n",
            "Written 7000 / 326670\n",
            "Written 8000 / 326670\n",
            "Written 9000 / 326670\n",
            "Written 10000 / 326670\n",
            "Written 11000 / 326670\n",
            "Written 12000 / 326670\n",
            "Written 13000 / 326670\n",
            "Written 14000 / 326670\n",
            "Written 15000 / 326670\n",
            "Written 16000 / 326670\n",
            "Written 17000 / 326670\n",
            "Written 18000 / 326670\n",
            "Written 19000 / 326670\n",
            "Written 20000 / 326670\n",
            "Written 21000 / 326670\n",
            "Written 22000 / 326670\n",
            "Written 23000 / 326670\n",
            "Written 24000 / 326670\n",
            "Written 25000 / 326670\n",
            "Written 26000 / 326670\n",
            "Written 27000 / 326670\n",
            "Written 28000 / 326670\n",
            "Written 29000 / 326670\n",
            "Written 30000 / 326670\n",
            "Written 31000 / 326670\n",
            "Written 32000 / 326670\n",
            "Written 33000 / 326670\n",
            "Written 34000 / 326670\n",
            "Written 35000 / 326670\n",
            "Written 36000 / 326670\n",
            "Written 37000 / 326670\n",
            "Written 38000 / 326670\n",
            "Written 39000 / 326670\n",
            "Written 40000 / 326670\n",
            "Written 41000 / 326670\n",
            "Written 42000 / 326670\n",
            "Written 43000 / 326670\n",
            "Written 44000 / 326670\n",
            "Written 45000 / 326670\n",
            "Written 46000 / 326670\n",
            "Written 47000 / 326670\n",
            "Written 48000 / 326670\n",
            "Written 49000 / 326670\n",
            "Written 50000 / 326670\n",
            "Written 51000 / 326670\n",
            "Written 52000 / 326670\n",
            "Written 53000 / 326670\n",
            "Written 54000 / 326670\n",
            "Written 55000 / 326670\n",
            "Written 56000 / 326670\n",
            "Written 57000 / 326670\n",
            "Written 58000 / 326670\n",
            "Written 59000 / 326670\n",
            "Written 60000 / 326670\n",
            "Written 61000 / 326670\n",
            "Written 62000 / 326670\n",
            "Written 63000 / 326670\n",
            "Written 64000 / 326670\n",
            "Written 65000 / 326670\n",
            "Written 66000 / 326670\n",
            "Written 67000 / 326670\n",
            "Written 68000 / 326670\n",
            "Written 69000 / 326670\n",
            "Written 70000 / 326670\n",
            "Written 71000 / 326670\n",
            "Written 72000 / 326670\n",
            "Written 73000 / 326670\n",
            "Written 74000 / 326670\n",
            "Written 75000 / 326670\n",
            "Written 76000 / 326670\n",
            "Written 77000 / 326670\n",
            "Written 78000 / 326670\n",
            "Written 79000 / 326670\n",
            "Written 80000 / 326670\n",
            "Written 81000 / 326670\n",
            "Written 82000 / 326670\n",
            "Written 83000 / 326670\n",
            "Written 84000 / 326670\n",
            "Written 85000 / 326670\n",
            "Written 86000 / 326670\n",
            "Written 87000 / 326670\n",
            "Written 88000 / 326670\n",
            "Written 89000 / 326670\n",
            "Written 90000 / 326670\n",
            "Written 91000 / 326670\n",
            "Written 92000 / 326670\n",
            "Written 93000 / 326670\n",
            "Written 94000 / 326670\n",
            "Written 95000 / 326670\n",
            "Written 96000 / 326670\n",
            "Written 97000 / 326670\n",
            "Written 98000 / 326670\n",
            "Written 99000 / 326670\n",
            "Written 100000 / 326670\n",
            "Written 101000 / 326670\n",
            "Written 102000 / 326670\n",
            "Written 103000 / 326670\n",
            "Written 104000 / 326670\n",
            "Written 105000 / 326670\n",
            "Written 106000 / 326670\n",
            "Written 107000 / 326670\n",
            "Written 108000 / 326670\n",
            "Written 109000 / 326670\n",
            "Written 110000 / 326670\n",
            "Written 111000 / 326670\n",
            "Written 112000 / 326670\n",
            "Written 113000 / 326670\n",
            "Written 114000 / 326670\n",
            "Written 115000 / 326670\n",
            "Written 116000 / 326670\n",
            "Written 117000 / 326670\n",
            "Written 118000 / 326670\n",
            "Written 119000 / 326670\n",
            "Written 120000 / 326670\n",
            "Written 121000 / 326670\n",
            "Written 122000 / 326670\n",
            "Written 123000 / 326670\n",
            "Written 124000 / 326670\n",
            "Written 125000 / 326670\n",
            "Written 126000 / 326670\n",
            "Written 127000 / 326670\n",
            "Written 128000 / 326670\n",
            "Written 129000 / 326670\n",
            "Written 130000 / 326670\n",
            "Written 131000 / 326670\n",
            "Written 132000 / 326670\n",
            "Written 133000 / 326670\n",
            "Written 134000 / 326670\n",
            "Written 135000 / 326670\n",
            "Written 136000 / 326670\n",
            "Written 137000 / 326670\n",
            "Written 138000 / 326670\n",
            "Written 139000 / 326670\n",
            "Written 140000 / 326670\n",
            "Written 141000 / 326670\n",
            "Written 142000 / 326670\n",
            "Written 143000 / 326670\n",
            "Written 144000 / 326670\n",
            "Written 145000 / 326670\n",
            "Written 146000 / 326670\n",
            "Written 147000 / 326670\n",
            "Written 148000 / 326670\n",
            "Written 149000 / 326670\n",
            "Written 150000 / 326670\n",
            "Written 151000 / 326670\n",
            "Written 152000 / 326670\n",
            "Written 153000 / 326670\n",
            "Written 154000 / 326670\n",
            "Written 155000 / 326670\n",
            "Written 156000 / 326670\n",
            "Written 157000 / 326670\n",
            "Written 158000 / 326670\n",
            "Written 159000 / 326670\n",
            "Written 160000 / 326670\n",
            "Written 161000 / 326670\n",
            "Written 162000 / 326670\n",
            "Written 163000 / 326670\n",
            "Written 164000 / 326670\n",
            "Written 165000 / 326670\n",
            "Written 166000 / 326670\n",
            "Written 167000 / 326670\n",
            "Written 168000 / 326670\n",
            "Written 169000 / 326670\n",
            "Written 170000 / 326670\n",
            "Written 171000 / 326670\n",
            "Written 172000 / 326670\n",
            "Written 173000 / 326670\n",
            "Written 174000 / 326670\n",
            "Written 175000 / 326670\n",
            "Written 176000 / 326670\n",
            "Written 177000 / 326670\n",
            "Written 178000 / 326670\n",
            "Written 179000 / 326670\n",
            "Written 180000 / 326670\n",
            "Written 181000 / 326670\n",
            "Written 182000 / 326670\n",
            "Written 183000 / 326670\n",
            "Written 184000 / 326670\n",
            "Written 185000 / 326670\n",
            "Written 186000 / 326670\n",
            "Written 187000 / 326670\n",
            "Written 188000 / 326670\n",
            "Written 189000 / 326670\n",
            "Written 190000 / 326670\n",
            "Written 191000 / 326670\n",
            "Written 192000 / 326670\n",
            "Written 193000 / 326670\n",
            "Written 194000 / 326670\n",
            "Written 195000 / 326670\n",
            "Written 196000 / 326670\n",
            "Written 197000 / 326670\n",
            "Written 198000 / 326670\n",
            "Written 199000 / 326670\n",
            "Written 200000 / 326670\n",
            "Written 201000 / 326670\n",
            "Written 202000 / 326670\n",
            "Written 203000 / 326670\n",
            "Written 204000 / 326670\n",
            "Written 205000 / 326670\n",
            "Written 206000 / 326670\n",
            "Written 207000 / 326670\n",
            "Written 208000 / 326670\n",
            "Written 209000 / 326670\n",
            "Written 210000 / 326670\n",
            "Written 211000 / 326670\n",
            "Written 212000 / 326670\n",
            "Written 213000 / 326670\n",
            "Written 214000 / 326670\n",
            "Written 215000 / 326670\n",
            "Written 216000 / 326670\n",
            "Written 217000 / 326670\n",
            "Written 218000 / 326670\n",
            "Written 219000 / 326670\n",
            "Written 220000 / 326670\n",
            "Written 221000 / 326670\n",
            "Written 222000 / 326670\n",
            "Written 223000 / 326670\n",
            "Written 224000 / 326670\n",
            "Written 225000 / 326670\n",
            "Written 226000 / 326670\n",
            "Written 227000 / 326670\n",
            "Written 228000 / 326670\n",
            "Written 229000 / 326670\n",
            "Written 230000 / 326670\n",
            "Written 231000 / 326670\n",
            "Written 232000 / 326670\n",
            "Written 233000 / 326670\n",
            "Written 234000 / 326670\n",
            "Written 235000 / 326670\n",
            "Written 236000 / 326670\n",
            "Written 237000 / 326670\n",
            "Written 238000 / 326670\n",
            "Written 239000 / 326670\n",
            "Written 240000 / 326670\n",
            "Written 241000 / 326670\n",
            "Written 242000 / 326670\n",
            "Written 243000 / 326670\n",
            "Written 244000 / 326670\n",
            "Written 245000 / 326670\n",
            "Written 246000 / 326670\n",
            "Written 247000 / 326670\n",
            "Written 248000 / 326670\n",
            "Written 249000 / 326670\n",
            "Written 250000 / 326670\n",
            "Written 251000 / 326670\n",
            "Written 252000 / 326670\n",
            "Written 253000 / 326670\n",
            "Written 254000 / 326670\n",
            "Written 255000 / 326670\n",
            "Written 256000 / 326670\n",
            "Written 257000 / 326670\n",
            "Written 258000 / 326670\n",
            "Written 259000 / 326670\n",
            "Written 260000 / 326670\n",
            "Written 261000 / 326670\n",
            "Written 262000 / 326670\n",
            "Written 263000 / 326670\n",
            "Written 264000 / 326670\n",
            "Written 265000 / 326670\n",
            "Written 266000 / 326670\n",
            "Written 267000 / 326670\n",
            "Written 268000 / 326670\n",
            "Written 269000 / 326670\n",
            "Written 270000 / 326670\n",
            "Written 271000 / 326670\n",
            "Written 272000 / 326670\n",
            "Written 273000 / 326670\n",
            "Written 274000 / 326670\n",
            "Written 275000 / 326670\n",
            "Written 276000 / 326670\n",
            "Written 277000 / 326670\n",
            "Written 278000 / 326670\n",
            "Written 279000 / 326670\n",
            "Written 280000 / 326670\n",
            "Written 281000 / 326670\n",
            "Written 282000 / 326670\n",
            "Written 283000 / 326670\n",
            "Written 284000 / 326670\n",
            "Written 285000 / 326670\n",
            "Written 286000 / 326670\n",
            "Written 287000 / 326670\n",
            "Written 288000 / 326670\n",
            "Written 289000 / 326670\n",
            "Written 290000 / 326670\n",
            "Written 291000 / 326670\n",
            "Written 292000 / 326670\n",
            "Written 293000 / 326670\n",
            "Written 294000 / 326670\n",
            "Written 295000 / 326670\n",
            "Written 296000 / 326670\n",
            "Written 297000 / 326670\n",
            "Written 298000 / 326670\n",
            "Written 299000 / 326670\n",
            "Written 300000 / 326670\n",
            "Written 301000 / 326670\n",
            "Written 302000 / 326670\n",
            "Written 303000 / 326670\n",
            "Written 304000 / 326670\n",
            "Written 305000 / 326670\n",
            "Written 306000 / 326670\n",
            "Written 307000 / 326670\n",
            "Written 308000 / 326670\n",
            "Written 309000 / 326670\n",
            "Written 310000 / 326670\n",
            "Written 311000 / 326670\n",
            "Written 312000 / 326670\n",
            "Written 313000 / 326670\n",
            "Written 314000 / 326670\n",
            "Written 315000 / 326670\n",
            "Written 316000 / 326670\n",
            "Written 317000 / 326670\n",
            "Written 318000 / 326670\n",
            "Written 319000 / 326670\n",
            "Written 320000 / 326670\n",
            "Written 321000 / 326670\n",
            "Written 322000 / 326670\n",
            "Written 323000 / 326670\n",
            "Written 324000 / 326670\n",
            "Written 325000 / 326670\n",
            "Written 326000 / 326670\n",
            "Created dataset with 326670 samples\n",
            "Written 1000 / 36297\n",
            "Written 2000 / 36297\n",
            "Written 3000 / 36297\n",
            "Written 4000 / 36297\n",
            "Written 5000 / 36297\n",
            "Written 6000 / 36297\n",
            "Written 7000 / 36297\n",
            "Written 8000 / 36297\n",
            "Written 9000 / 36297\n",
            "Written 10000 / 36297\n",
            "Written 11000 / 36297\n",
            "Written 12000 / 36297\n",
            "Written 13000 / 36297\n",
            "Written 14000 / 36297\n",
            "Written 15000 / 36297\n",
            "Written 16000 / 36297\n",
            "Written 17000 / 36297\n",
            "Written 18000 / 36297\n",
            "Written 19000 / 36297\n",
            "Written 20000 / 36297\n",
            "Written 21000 / 36297\n",
            "Written 22000 / 36297\n",
            "Written 23000 / 36297\n",
            "Written 24000 / 36297\n",
            "Written 25000 / 36297\n",
            "Written 26000 / 36297\n",
            "Written 27000 / 36297\n",
            "Written 28000 / 36297\n",
            "Written 29000 / 36297\n",
            "Written 30000 / 36297\n",
            "Written 31000 / 36297\n",
            "Written 32000 / 36297\n",
            "Written 33000 / 36297\n",
            "Written 34000 / 36297\n",
            "Written 35000 / 36297\n",
            "Written 36000 / 36297\n",
            "Created dataset with 36297 samples\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 현재 작업 디렉토리를 확인하고, 필요하다면 변경합니다.\n",
        "current_dir = os.getcwd()\n",
        "print(f\"Current working directory: {current_dir}\")\n",
        "\n",
        "# 만약 현재 디렉토리가 예상과 다르다면, 다음 줄의 주석을 해제하고 경로를 맞춰주세요.\n",
        "# os.chdir('E:\\\\download\\\\deep-text-recognition-benchmark')\n",
        "# print(f\"Changed working directory to: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# 출력 디렉토리 생성\n",
        "os.makedirs('./data_lmdb/train/basic', exist_ok=True)\n",
        "os.makedirs('./data_lmdb/validation/basic', exist_ok=True)\n",
        "\n",
        "# train LMDB 재생성\n",
        "!python3 E:\\download\\deep-text-recognition-benchmark\\create_lmdb_dataset.py \\\n",
        "  --inputPath E:\\download\\final_dataset\\train_data \\\n",
        "  --gtFile E:\\download\\final_dataset\\train_data\\labels.txt \\\n",
        "  --outputPath ./data_lmdb/train/basic\n",
        "\n",
        "# valid LMDB 재생성\n",
        "!python3 E:\\download\\deep-text-recognition-benchmark\\create_lmdb_dataset.py \\\n",
        "  --inputPath E:\\download\\final_dataset\\valid_data \\\n",
        "  --gtFile E:\\download\\final_dataset\\valid_data\\labels.txt \\\n",
        "  --outputPath ./data_lmdb/validation/basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 성공! 'korean_char_1008.txt' 파일이 올바른 형식으로 생성되었습니다.\n",
            "총 문자 수: 1008\n",
            "생성된 문자열 미리보기:  !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcde...\n"
          ]
        }
      ],
      "source": [
        "import ast # 이 라이브러리는 문자열을 안전하게 파이썬 객체로 변환합니다.\n",
        "\n",
        "input_filename = 'chars.txt'\n",
        "output_filename = 'korean_char_1008.txt'\n",
        "\n",
        "try:\n",
        "    # 1. 현재 잘못된 형식의 파일을 읽습니다.\n",
        "    with open(input_filename, 'r', encoding='utf-8') as f:\n",
        "        # 파일 내용을 하나의 문자열로 읽어옵니다.\n",
        "        content = f.read().replace('\\n', '')\n",
        "\n",
        "    # 2. 읽어온 문자열을 실제 파이썬 리스트로 변환합니다.\n",
        "    char_list = ast.literal_eval(content)\n",
        "\n",
        "    # 3. (중요) 리스트의 첫 요소인 '[blank]'를 제거합니다.\n",
        "    # [blank] 토큰은 학습 스크립트가 내부적으로 자동 추가하므로,\n",
        "    # 문자 리스트 파일에는 포함시키지 않아야 합니다.\n",
        "    if char_list and char_list[0] == '[blank]':\n",
        "        char_list = char_list[1:]\n",
        "\n",
        "    # 4. 문자 리스트를 하나의 순수한 문자열로 합칩니다.\n",
        "    correct_string = \"\".join(char_list)\n",
        "\n",
        "    # 5. 변환된 문자열을 새로운 파일에 저장합니다.\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(correct_string)\n",
        "\n",
        "    print(f\"✅ 성공! '{output_filename}' 파일이 올바른 형식으로 생성되었습니다.\")\n",
        "    print(f\"총 문자 수: {len(correct_string)}\")\n",
        "    print(f\"생성된 문자열 미리보기: {correct_string[:70]}...\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ 오류: '{input_filename}' 파일을 찾을 수 없습니다. 파일 이름을 확인해주세요.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 오류: 파일 변환 중 문제가 발생했습니다. - {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "dataset_root: ./data_lmdb/train/basic\n",
            "opt.select_data: ['basic']\n",
            "opt.batch_ratio: ['1']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    ./data_lmdb/train/basic\t dataset: basic\n",
            "sub-directory:\t/.\t num samples: 326670\n",
            "num total samples of basic: 326670 x 1.0 (total_data_usage_ratio) = 326670\n",
            "num samples of basic per batch: 16 x 1.0 (batch_ratio) = 16\n",
            "--------------------------------------------------------------------------------\n",
            "Total_batch_size: 16 = 16\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    ./data_lmdb/validation/basic\t dataset: /\n",
            "sub-directory:\t/.\t num samples: 36297\n",
            "--------------------------------------------------------------------------------\n",
            "model input parameters 64 100 20 1 256 256 23 25 TPS ResNet BiLSTM CTC\n",
            "Skip Transformation.LocalizationNetwork.localization_fc2.weight as it is already initialized\n",
            "Skip Transformation.LocalizationNetwork.localization_fc2.bias as it is already initialized\n",
            "loading pretrained model from E:/download/saved_models/korean_g2.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Traceback (most recent call last):\n",
            "  File \"e:\\download\\deep-text-recognition-benchmark\\train.py\", line 318, in <module>\n",
            "    train(opt)\n",
            "  File \"e:\\download\\deep-text-recognition-benchmark\\train.py\", line 84, in train\n",
            "    model.load_state_dict(torch.load(opt.saved_model, map_location=torch.device('cpu')), strict=False)\n",
            "  File \"C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py\", line 2593, in load_state_dict\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Error(s) in loading state_dict for DataParallel:\n",
            "\tsize mismatch for module.Prediction.weight: copying a param with shape torch.Size([1009, 256]) from checkpoint, the shape in current model is torch.Size([23, 256]).\n",
            "\tsize mismatch for module.Prediction.bias: copying a param with shape torch.Size([1009]) from checkpoint, the shape in current model is torch.Size([23]).\n"
          ]
        }
      ],
      "source": [
        "!python3 ./deep-text-recognition-benchmark/train.py --train_data ./data_lmdb/train/basic --valid_data ./data_lmdb/validation/basic --select_data basic --batch_ratio 1 --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction CTC --saved_model \"E:/download/saved_models/korean_g2.pth\" --FT --imgH 64 --imgW 100 --input_channel 1 --output_channel 256 --hidden_size 256 --data_filtering_off --batch_size 16 --workers 0 --num_iter 1000 --valInterval 100 --lr 0.001 --character E:/download/korean_char_1008.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ATXSA07AEDx",
        "outputId": "f6932dbe-cc5a-4bed-f518-5a9983c02264"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "> was unexpected at this time.\n"
          ]
        }
      ],
      "source": [
        "!python3 ./deep-text-recognition-benchmark/train.py \\\n",
        "--train_data ./data_lmdb/train/basic \\\n",
        "--valid_data ./data_lmdb/validation/basic \\\n",
        "--select_data basic \\\n",
        "--batch_ratio 1 \\\n",
        "--Transformation TPS \\\n",
        "--FeatureExtraction ResNet \\\n",
        "--SequenceModeling BiLSTM \\\n",
        "--Prediction CTC \\\n",
        "--imgH 64 \\\n",
        "--imgW 100 \\\n",
        "--input_channel 1 \\\n",
        "--output_channel 256 \\\n",
        "--hidden_size 256 \\\n",
        "--data_filtering_off \\\n",
        "--batch_size 16 \\\n",
        "--workers 0 \\\n",
        "--num_iter 1000 \\\n",
        "--valInterval 100 \\\n",
        "--saved_model \"E:/download/saved_models/korean_g2.pth\" \\\n",
        "--FT \\\n",
        "--lr 0.001 \\\n",
        "--character=\"!\\\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~가각간갇갈감갑값강갖같갚갛개객걀거걱건걷걸검겁것겉게겨격겪견결겹경곁계고곡곤곧골곰곱곳공과관광괜괴굉교구국군굳굴굵굶굽궁권귀규균그극근글긁금급긋긍기긴길김깅깊까깎깐깔깜깝깥깨꺼꺾껍껏껑께껴꼬꼭꼴꼼꼽꽂꽃꽉꽤꾸꿀꿈뀌끄끈끊끌끓끔끗끝끼낌나낙낚난날낡남납낫낭낮낯낱낳내냄냉냐냥너넉널넓넘넣네넥넷녀녁년념녕노녹논놀놈농높놓놔뇌뇨누눈눕뉘뉴늄느늑는늘늙능늦늬니닐님다닥닦단닫달닭닮담답닷당닿대댁댐더덕던덜덤덥덧덩덮데델도독돈돌돕동돼되된두둑둘둠둡둥뒤뒷드득든듣들듬듭듯등디딩딪따딱딴딸땀땅때땜떠떡떤떨떻떼또똑뚜뚫뚱뛰뜨뜩뜯뜰뜻띄라락란람랍랑랗래랜램랫략량러럭런럴럼럽럿렁렇레렉렌려력련렬렵령례로록론롬롭롯료루룩룹룻뤄류륙률륭르른름릇릎리릭린림립릿마막만많말맑맘맙맛망맞맡맣매맥맨맵맺머먹먼멀멈멋멍멎메멘멩며면멸명몇모목몰몸몹못몽묘무묵묶문묻물뭄뭇뭐뭣므미민믿밀밉밌및밑바박밖반받발밝밟밤밥방밭배백뱀뱃뱉버번벌범법벗베벤벼벽변별볍병볕보복볶본볼봄봇봉뵈뵙부북분불붉붐붓붕붙뷰브블비빌빗빚빛빠빨빵빼뺨뻐뻔뻗뼈뽑뿌뿐쁘쁨사삭산살삶삼상새색샌생서석섞선설섬섭섯성세센셈셋션소속손솔솜솟송솥쇄쇠쇼수숙순술숨숫숲쉬쉽슈스슨슬슴습슷승시식신싣실싫심십싱싶싸싹쌀쌍쌓써썩썰썹쎄쏘쏟쑤쓰쓸씀씌씨씩씬씹씻아악안앉않알앓암압앗앙앞애액야약얇양얗얘어억언얹얻얼엄업없엇엉엌엎에엔엘여역연열엷염엽엿영옆예옛오옥온올옮옳옷와완왕왜왠외왼요욕용우욱운울움웃웅워원월웨웬위윗유육율으윽은을음응의이익인일읽잃임입잇있잊잎자작잔잖잘잠잡장잦재쟁저적전절젊점접젓정젖제젠젯져조족존졸좀좁종좋좌죄주죽준줄줌줍중쥐즈즉즌즐즘증지직진질짐집짓징짙짚짜짝짧째쨌쩌쩍쩐쪽쫓쭈쭉찌찍찢차착찬찮찰참창찾채책챔챙처척천철첫청체쳐초촉촌총촬최추축춘출춤춥춧충취츠측츰층치칙친칠침칭카칸칼캐캠커컨컬컴컵컷켓켜코콜콤콩쾌쿠퀴크큰클큼키킬타탁탄탈탑탓탕태택탤터턱털텅테텍텔템토톤톱통퇴투툼퉁튀튜트특튼튿틀틈티틱팀팅파팎판팔패팩팬퍼퍽페펴편펼평폐포폭표푸푹풀품풍퓨프플픔피픽필핏핑하학한할함합항해핵핸햄햇행향허헌험헤헬혀현혈협형혜호혹혼홀홍화확환활황회획횟효후훈훌훔훨휘휴흉흐흑흔흘흙흡흥흩희흰히힘\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "iWgcj9m-JDIE",
        "outputId": "467e2224-9a3b-4870-9010-a264aad885ee"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3689481335.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlmdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data_lmdb/train/basic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnSamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num-samples'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of samples: {nSamples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
          ]
        }
      ],
      "source": [
        "import lmdb\n",
        "# Check if the LMDB file can be opened\n",
        "env = lmdb.open('./data_lmdb/train/basic')\n",
        "with env.begin(write=False) as txn:\n",
        "    nSamples = int(txn.get('num-samples'.encode()))\n",
        "    print(f\"Number of samples: {nSamples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9SqMbc9BjiY"
      },
      "outputs": [],
      "source": [
        "!ls ./data_lmdb/train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppli6Qh6id7B",
        "outputId": "61f301cc-fd08-4193-9d2d-1574aed8c371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/content/deep-text-recognition-benchmark/data/gt_train.txt' 파일의 확장자를 .PNG로 성공적으로 변경했습니다.\n",
            "'/content/deep-text-recognition-benchmark/data/gt_valid.txt' 파일의 확장자를 .PNG로 성공적으로 변경했습니다.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 정확한 전체 파일 경로로 수정\n",
        "file_paths = [\n",
        "    '/content/deep-text-recognition-benchmark/data/gt_train.txt',\n",
        "    '/content/deep-text-recognition-benchmark/data/gt_valid.txt'\n",
        "]\n",
        "\n",
        "def change_extension_case(file_path):\n",
        "    \"\"\"\n",
        "    파일을 읽어 '.png'를 '.PNG'로 변경한 뒤 다시 저장합니다.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 파일 읽기\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # '.png'가 있는지 확인하고 변경\n",
        "        if '.png' in content:\n",
        "            new_content = content.replace('.png', '.PNG')\n",
        "\n",
        "            # 변경된 내용으로 파일 다시 쓰기\n",
        "            with open(file_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(new_content)\n",
        "            print(f\"'{file_path}' 파일의 확장자를 .PNG로 성공적으로 변경했습니다.\")\n",
        "        else:\n",
        "            print(f\"'{file_path}' 파일에 변경할 '.png' 확장자가 없습니다.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"오류: '{file_path}' 파일을 찾을 수 없습니다. 파일 경로를 다시 확인해주세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\"'{file_path}' 파일을 처리하는 중 오류가 발생했습니다: {e}\")\n",
        "\n",
        "# 목록에 있는 각 파일에 대해 함수 실행\n",
        "for path in file_paths:\n",
        "    change_extension_case(path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uyUGMugOkd1",
        "outputId": "cad57a21-59b9-4122-9043-46b1e5c9c515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 모든 고유 글자를 성공적으로 추출했습니다.\n",
            "--------------------------------------------------\n",
            "생성된 character 문자열:\n",
            " ()-.0123456789:m가감같개게겼계고과광구균금기길김까낙납년농는다달대덕도동등때래로록를만면명문받방배백번보본부북사산삼서성스승시신쌍아앙액약어업에역예오와완용우원월위으은을의이인일임자전정제조주죽중증지진차창천철체최카태터트파팔하한함합해현협호화훈\n",
            "--------------------------------------------------\n",
            "👇 아래의 --character 옵션을 복사하여 학습 명령어에 추가하세요.\n",
            "--character=\" ()-.0123456789:m가감같개게겼계고과광구균금기길김까낙납년농는다달대덕도동등때래로록를만면명문받방배백번보본부북사산삼서성스승시신쌍아앙액약어업에역예오와완용우원월위으은을의이인일임자전정제조주죽중증지진차창천철체최카태터트파팔하한함합해현협호화훈\"\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# 레이블 파일 경로 (사용자 환경에 맞게 수정)\n",
        "gt_file_path = '/content/deep-text-recognition-benchmark/data/gt_train.txt'\n",
        "\n",
        "def generate_character_list(file_path):\n",
        "    \"\"\"\n",
        "    레이블 파일(gt.txt)을 읽어 모든 고유 글자를 추출하고,\n",
        "    학습에 사용할 수 있는 character 문자열을 생성합니다.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        # 각 줄에서 탭(\\t)으로 분리된 텍스트 부분만 추출\n",
        "        labels = [line.strip().split('\\t')[1] for line in lines if '\\t' in line]\n",
        "\n",
        "        # 모든 텍스트를 하나로 합친 뒤, 고유한 글자만 추출\n",
        "        all_text = \"\".join(labels)\n",
        "        unique_characters = sorted(list(set(all_text)))\n",
        "\n",
        "        # 학습 스크립트에 사용할 최종 문자열 생성\n",
        "        character_string = \"\".join(unique_characters)\n",
        "\n",
        "        print(\"✅ 모든 고유 글자를 성공적으로 추출했습니다.\")\n",
        "        print(\"-\" * 50)\n",
        "        print(\"생성된 character 문자열:\")\n",
        "        print(character_string)\n",
        "        print(\"-\" * 50)\n",
        "        print(\"👇 아래의 --character 옵션을 복사하여 학습 명령어에 추가하세요.\")\n",
        "        print(f'--character=\"{character_string}\"')\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ 오류: '{file_path}' 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\"오류가 발생했습니다: {e}\")\n",
        "\n",
        "# 함수 실행\n",
        "generate_character_list(gt_file_path)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
